name: ibook-mlops

services:
  postgres:
    image: postgres:15-alpine
    container_name: ibook-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-ibook}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-ibook}
      POSTGRES_DB: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/01-init-db.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d postgres"]
      interval: 5s
      timeout: 3s
      retries: 20

  redis:
    image: redis:7-alpine
    container_name: ibook-redis
    command: ["sh", "-c", "redis-server --appendonly yes ${REDIS_PASSWORD:+--requirepass $REDIS_PASSWORD}"]
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 20

  minio:
    image: minio/minio:RELEASE.2025-01-20T14-49-07Z
    container_name: ibook-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY:-minioadmin}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 5s
      timeout: 3s
      retries: 20

  minio-init:
    image: minio/mc:RELEASE.2025-05-21T01-59-54Z-cpuv1
    container_name: ibook-minio-init
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      # Always use Docker service name; .env may set localhost for host-side access
      MINIO_ENDPOINT: http://minio:9000
      MINIO_BUCKET: ${MINIO_BUCKET:-mlflow}
      RAW_EVENTS_BUCKET: ${RAW_EVENTS_BUCKET:-raw-events}
    entrypoint:
      - sh
      - -c
      - |
        for i in 1 2 3 4 5 6 7 8 9 10; do
          if mc alias set local "$${MINIO_ENDPOINT}" "$${MINIO_ACCESS_KEY}" "$${MINIO_SECRET_KEY}" 2>/dev/null; then
            mc mb --ignore-existing "local/$${MINIO_BUCKET}"
            mc mb --ignore-existing "local/$${RAW_EVENTS_BUCKET}"
            exit 0
          fi
          echo "Waiting for MinIO API... attempt $$i/10"
          sleep 2
        done
        echo "MinIO init failed: could not connect"
        exit 1

  mlflow:
    build:
      context: .
      dockerfile: services/mlflow/Dockerfile
    container_name: ibook-mlflow
    depends_on:
      postgres:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      # Use Docker service name so MLflow in container can reach MinIO
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      MLFLOW_BACKEND_STORE_URI: postgresql+psycopg2://${POSTGRES_USER:-ibook}:${POSTGRES_PASSWORD:-ibook}@postgres:5432/${POSTGRES_MLFLOW_DB:-mlflow}
      MLFLOW_DEFAULT_ARTIFACT_ROOT: s3://${MINIO_BUCKET:-mlflow}/
    ports:
      - "5000:5000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/healthz"]
      interval: 10s
      timeout: 5s
      retries: 12

  airflow-webserver:
    build:
      context: .
      dockerfile: services/airflow/Dockerfile
    image: ibook-airflow:latest
    container_name: ibook-airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-ibook}:${POSTGRES_PASSWORD:-ibook}@postgres:5432/${POSTGRES_AIRFLOW_DB:-airflow}
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER_SECRET_KEY:-airflow-webserver-secret-key}
      PYTHONPATH: /opt/airflow/workspace
      AIRFLOW_TASK_RETRIES: "${AIRFLOW_TASK_RETRIES:-3}"
      FEATURE_ENGINEERING_PIPELINE_RETRIES: "${FEATURE_ENGINEERING_PIPELINE_RETRIES:-}"
      MODEL_TRAINING_PIPELINE_RETRIES: "${MODEL_TRAINING_PIPELINE_RETRIES:-}"
      ML_MONITORING_PIPELINE_RETRIES: "${ML_MONITORING_PIPELINE_RETRIES:-}"
      BENTOML_BASE_URL: ${BENTOML_BASE_URL:-http://bentoml-fraud:7001}
      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      AWS_S3_ENDPOINT_URL: http://minio:9000
      RAW_EVENTS_BUCKET: ${RAW_EVENTS_BUCKET:-raw-events}
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
    ports:
      - "8080:8080"
    volumes:
      - ./services/airflow/dags:/opt/airflow/dags:ro
      - ./services/airflow/plugins:/opt/airflow/plugins:ro
      - .:/opt/airflow/workspace:ro
      # Writable data dir so feature_engineering_pipeline can write Feast parquet files into data/processed/feast
      - ./data:/opt/airflow/workspace/data
    command: >
      bash -c "
      airflow db migrate &&
      airflow users create --role Admin --username $$(_AIRFLOW_WWW_USER_USERNAME) --password $$(_AIRFLOW_WWW_USER_PASSWORD) --firstname Admin --lastname User --email admin@example.com || true &&
      airflow webserver"

  airflow-scheduler:
    image: ibook-airflow:latest
    container_name: ibook-airflow-scheduler
    depends_on:
      airflow-webserver:
        condition: service_started
      mlflow:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-ibook}:${POSTGRES_PASSWORD:-ibook}@postgres:5432/${POSTGRES_AIRFLOW_DB:-airflow}
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER_SECRET_KEY:-airflow-webserver-secret-key}
      PYTHONPATH: /opt/airflow/workspace
      AIRFLOW_TASK_RETRIES: "${AIRFLOW_TASK_RETRIES:-3}"
      FEATURE_ENGINEERING_PIPELINE_RETRIES: "${FEATURE_ENGINEERING_PIPELINE_RETRIES:-}"
      MODEL_TRAINING_PIPELINE_RETRIES: "${MODEL_TRAINING_PIPELINE_RETRIES:-}"
      ML_MONITORING_PIPELINE_RETRIES: "${ML_MONITORING_PIPELINE_RETRIES:-}"
      MLFLOW_TRACKING_URI: http://mlflow:5000
      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      BENTOML_BASE_URL: ${BENTOML_BASE_URL:-http://bentoml-fraud:7001}
      RAW_EVENTS_BUCKET: ${RAW_EVENTS_BUCKET:-raw-events}
    volumes:
      - ./services/airflow/dags:/opt/airflow/dags:ro
      - ./services/airflow/plugins:/opt/airflow/plugins:ro
      - .:/opt/airflow/workspace:ro
      # Writable data dir so feature_engineering_pipeline can write Feast parquet files into data/processed/feast
      - ./data:/opt/airflow/workspace/data
    command: ["bash", "-c", "airflow scheduler"]

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    container_name: ibook-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      # Enable 4LW commands so cp-kafka preflight can use "ruok".
      KAFKA_OPTS: -Dzookeeper.4lw.commands.whitelist=ruok,srvr,mntr,stat,conf,isro
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD-SHELL", "echo srvr | nc -w 2 localhost 2181 | grep -q Mode"]
      interval: 10s
      timeout: 5s
      retries: 12

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: ibook-kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # Expose two listeners:
      # - PLAINTEXT://localhost:9092 for host access
      # - INTERNAL://kafka:29092 for other containers
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,INTERNAL://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,INTERNAL://kafka:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 12

  kafka-init:
    image: confluentinc/cp-kafka:7.6.0
    container_name: ibook-kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint:
      - sh
      - -c
      - |
        echo "Waiting for Kafka broker at kafka:29092..."
        for i in 1 2 3 4 5 6 7 8 9 10; do
          /usr/bin/kafka-topics --bootstrap-server kafka:29092 --list >/dev/null 2>&1 && break
          echo "Kafka not ready yet (attempt $$i/10); sleeping..."
          sleep 5
        done
        echo "Creating topic raw.transactions (if not exists)..."
        /usr/bin/kafka-topics \
          --bootstrap-server kafka:29092 \
          --create \
          --if-not-exists \
          --topic raw.transactions \
          --partitions 3 \
          --replication-factor 1 || true
        echo "Kafka topic initialization complete."

  parquet-sink:
    build:
      context: .
      dockerfile: services/parquet_sink/Dockerfile
    container_name: ibook-parquet-sink
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
      minio-init:
        condition: service_completed_successfully
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_RAW_TOPIC: raw.transactions
      KAFKA_GROUP_ID: parquet-sink
      PARQUET_SINK_BATCH_SIZE: "500"
      PARQUET_SINK_FLUSH_INTERVAL_SEC: "30"
      RAW_EVENTS_BUCKET: ${RAW_EVENTS_BUCKET:-raw-events}
      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      AWS_S3_ENDPOINT_URL: http://minio:9000

  faust-worker:
    build:
      context: .
      dockerfile: services/faust_worker/Dockerfile
    container_name: ibook-faust-worker
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_RAW_TOPIC: raw.transactions
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      FEAST_REPO_PATH: /app/services/feast/feature_repo
      FAUST_PUSH_INTERVAL_SEC: "30"

  prometheus:
    image: prom/prometheus:v2.49.1
    container_name: ibook-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./services/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./services/monitoring/prometheus/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro

  grafana:
    image: grafana/grafana:10.3.1
    container_name: ibook-grafana
    ports:
      - "3000:3000"
    volumes:
      - ./services/monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
      - ./services/monitoring/grafana/dashboards/dashboard_provider.yml:/etc/grafana/provisioning/dashboards/default.yaml:ro
      - ./services/monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro

  jupyter:
    build:
      context: .
      dockerfile: services/jupyter/Dockerfile
    container_name: ibook-jupyter
    ports:
      - "8888:8888"
    volumes:
      - ./:/workspace
    working_dir: /workspace

  bentoml-fraud:
    build:
      context: .
      dockerfile: services/bentoml/services/fraud_detection/Dockerfile
    container_name: ibook-bentoml-fraud
    depends_on:
      mlflow:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      ENVIRONMENT: ${ENVIRONMENT:-local}
      MLFLOW_TRACKING_URI: http://mlflow:5000
      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      REDIS_HOST: redis
      REDIS_PORT: 6379
    ports:
      - "7001:7001"
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sf",
          "-X",
          "POST",
          "-H",
          "Content-Type: application/json",
          "-d",
          "{}",
          "http://localhost:7001/health",
        ]
      interval: 10s
      timeout: 5s
      retries: 12

  bentoml-pricing:
    build:
      context: .
      dockerfile: services/bentoml/services/dynamic_pricing/Dockerfile
    container_name: ibook-bentoml-pricing
    depends_on:
      redis:
        condition: service_healthy
    environment:
      ENVIRONMENT: ${ENVIRONMENT:-local}
      REDIS_HOST: redis
      REDIS_PORT: 6379
    ports:
      - "7002:7002"
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sf",
          "-X",
          "POST",
          "-H",
          "Content-Type: application/json",
          "-d",
          "{}",
          "http://localhost:7002/health",
        ]
      interval: 10s
      timeout: 5s
      retries: 12

volumes:
  postgres_data:
  redis_data:
  minio_data:

